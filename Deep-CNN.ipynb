{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from random import shuffle\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = './dataset/'\n",
    "\n",
    "face_path  = dataset_folder + 'face'\n",
    "nface_path = dataset_folder + 'nonface'\n",
    "testing_face_path = dataset_folder + 'testing/face'\n",
    "testing_nface_path = dataset_folder + 'testing/nonface'\n",
    "img_ext = ['.jpg', '.png', 'jpeg', '.pgm', '.bmp', 'JPEG']\n",
    "\n",
    "height, width = 128, 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_face_files(file_path=face_path):\n",
    "    face_list = []\n",
    "    \n",
    "    for f in glob.glob(file_path + '/**', recursive=True):\n",
    "        \n",
    "        if f[-4:] in img_ext:\n",
    "            face_list.append(f)\n",
    "    \n",
    "    return face_list\n",
    "\n",
    "def read_nonface_files(file_path=nface_path):\n",
    "    \n",
    "    nonface_list = []\n",
    "    \n",
    "    for f in glob.glob(file_path + '/**', recursive=True):\n",
    "        \n",
    "        if f[-4:] in img_ext:\n",
    "            nonface_list.append(f)\n",
    "    \n",
    "    return nonface_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_list = read_face_files()\n",
    "nonface_list = read_nonface_files()\n",
    "\n",
    "num_faces = len(face_list)\n",
    "num_nonface = len(nonface_list)\n",
    "\n",
    "face_label = [0] * num_faces\n",
    "nonface_label = [1] * num_nonface\n",
    "\n",
    "data_list = face_list + nonface_list\n",
    "label_list = face_label + nonface_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_list(list1, list2):\n",
    "    \n",
    "    combined = list(zip(list1, list2))\n",
    "    shuffle(combined)\n",
    "\n",
    "    list1, list2 = zip(*combined)\n",
    "\n",
    "    return (list1, list2)\n",
    "\n",
    "data_list, label_list = shuffle_list(data_list, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(data_list=data_list, label_list=label_list):\n",
    "    image_list = []\n",
    "    one_hot_list = []\n",
    "    for d in data_list:\n",
    "        \n",
    "        img = cv2.imread(d, 0)\n",
    "        img = cv2.resize(img, (height,width))\n",
    "        img = np.asarray(img, dtype='float32')/255\n",
    "        img = np.reshape(img, (height, width, 1))\n",
    "        image_list.append(img)\n",
    "    \n",
    "    for l in label_list:\n",
    "        \n",
    "        zeros = np.zeros((2))\n",
    "        zeros[int(l)] = 1.0\n",
    "        one_hot_list.append(zeros)\n",
    "    \n",
    "    labels = np.asarray(one_hot_list, dtype='float32')\n",
    "    images = np.asarray(image_list, dtype='float32')\n",
    "    \n",
    "    return (images, labels)\n",
    "\n",
    "images, labels = read_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_list = read_face_files(testing_face_path)\n",
    "nonface_list = read_nonface_files(testing_nface_path)\n",
    "\n",
    "num_faces = len(face_list)\n",
    "num_nonface = len(nonface_list)\n",
    "\n",
    "face_label = [0] * num_faces\n",
    "nonface_label = [1] * num_nonface\n",
    "\n",
    "data_list = face_list + nonface_list\n",
    "label_list = face_label + nonface_label\n",
    "\n",
    "data_list, label_list = shuffle_list(data_list, label_list)\n",
    "\n",
    "test_images,test_labels = read_images(data_list, label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate #to print the output of our model in a tabular manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size           = 5\n",
    "learning_rate        = 1e-4\n",
    "epoch                = 20\n",
    "num_labels           = 2\n",
    "dropout_rate         = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-86ebbb3e0d8a>:47: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, height, width, 1), name='input')\n",
    "Y = tf.placeholder(tf.float32, shape=(None, num_labels), name='target')\n",
    "dropout_prob = tf.placeholder(tf.float32, name='dropout')\n",
    "\n",
    "\n",
    "conv1 = tf.contrib.layers.conv2d(X, num_outputs=128, kernel_size=3, stride=1, padding='SAME',\n",
    "                                 activation_fn=tf.nn.relu, scope='Conv_1')\n",
    "\n",
    "conv1_pool = max_pool_2x2(conv1)\n",
    "\n",
    "conv2 = tf.contrib.layers.conv2d(conv1_pool, num_outputs=256, kernel_size=3, stride=1, padding='SAME',\n",
    "                                 activation_fn=tf.nn.relu, scope='Conv_2')\n",
    "conv3 = tf.contrib.layers.conv2d(conv2, num_outputs=256, kernel_size=5, stride=1, padding='SAME',\n",
    "                                 activation_fn=tf.nn.relu, scope='Conv_3')\n",
    "\n",
    "conv2_pool = max_pool_2x2(conv3)\n",
    "\n",
    "conv4 = tf.contrib.layers.conv2d(conv2_pool, num_outputs=512, kernel_size=5, stride=1, padding='SAME',\n",
    "                                 activation_fn=tf.nn.relu, scope='Conv_4')\n",
    "\n",
    "conv5 = tf.contrib.layers.conv2d(conv4, num_outputs=512, kernel_size=5, stride=1, padding='SAME',\n",
    "                                 activation_fn=tf.nn.relu, scope='Conv_5')\n",
    "\n",
    "conv5_pool = max_pool_2x2(conv5)\n",
    "\n",
    "conv6 = tf.contrib.layers.conv2d(conv5_pool, num_outputs=512, kernel_size=3, stride=1, padding='SAME',\n",
    "                                 activation_fn=tf.nn.relu, scope='Conv_6')\n",
    "\n",
    "conv6_pool = max_pool_2x2(conv6)\n",
    "\n",
    "\n",
    "W_fc1 = weight_variable([8 * 8 * 512, 512])\n",
    "b_fc1 = bias_variable([512])\n",
    "\n",
    "h_pool3_flat = tf.reshape(conv6_pool, [-1, 8*8*512])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)\n",
    "\n",
    "fc1_dropout = tf.nn.dropout(h_fc1, dropout_prob)\n",
    "\n",
    "W_fc2 = weight_variable([512, num_labels])\n",
    "b_fc2 = bias_variable([num_labels])\n",
    "\n",
    "y_conv = tf.add(tf.matmul(fc1_dropout, W_fc2), b_fc2, name='output')\n",
    "\n",
    "\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=y_conv),name='loss')\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30905090\n"
     ]
    }
   ],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "#     print(shape)\n",
    "#     print(len(shape))\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "#         print(dim)\n",
    "        variable_parameters *= dim.value\n",
    "#     print(variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./cnn_ckpt/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3880 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is not loaded !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 2366/3880 [17:38<10:43,  2.35it/s]"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "writer = tf.summary.FileWriter('./tensorboard')\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "#to load a saved model\n",
    "try:    \n",
    "    saver.restore(sess, './cnn_ckpt/model.ckpt')\n",
    "    print(\"Model is loaded!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Model is not loaded !\")\n",
    "    pass\n",
    "\n",
    "total_images = images.shape[0] #total number of training images\n",
    "total_test_images = test_images.shape[0] #total number of testing images\n",
    "\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch_iter in range(epoch):\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    test_total_loss = 0\n",
    "    test_total_accuracy = 0\n",
    "    counter = 0\n",
    "    test_counter= 0\n",
    "    \n",
    "    for index in tqdm(range(0, total_images, batch_size)):\n",
    "        \n",
    "        end_batch = index + batch_size\n",
    "        \n",
    "        if end_batch >= total_images : end_batch = None #to prevent the last index to go beyond the data size\n",
    "            \n",
    "        loss_, accuracy_, _ = sess.run([cross_entropy, accuracy, train_step],\n",
    "                                                  feed_dict={X:images[index:end_batch], \n",
    "                                                             Y:labels[index:end_batch],\n",
    "                                                             dropout_prob:dropout_rate})\n",
    "        total_loss += loss_\n",
    "        total_accuracy += accuracy_\n",
    "        \n",
    "        counter += 1\n",
    "    \n",
    "    for test_index in tqdm(range(0, total_test_images, batch_size)):\n",
    "        \n",
    "        end_batch_test = test_index + batch_size \n",
    "        \n",
    "        if end_batch_test >= total_test_images : end_batch_test = None #to prevent the last index to go beyond the data size\n",
    "        \n",
    "        loss_, accuracy_ = sess.run([cross_entropy, accuracy],\n",
    "                                                  feed_dict={X:test_images[test_index:end_batch_test], \n",
    "                                                             Y:test_labels[test_index:end_batch_test],\n",
    "                                                             dropout_prob:1.0})\n",
    "        \n",
    "        test_total_loss += loss_\n",
    "        test_total_accuracy += accuracy_\n",
    "        test_counter += 1\n",
    "        \n",
    "    print(\"\\t----Epoch %d---- \"%(epoch_iter))\n",
    "    print(tabulate([[\"Loss\", total_loss],[\"Classification Accuracy\", total_accuracy/counter]],\n",
    "                       headers=[\"Training Loss/Accuracy\", \"Value\"]))\n",
    "    print(\"\\n\")\n",
    "    print(tabulate([[\"Loss\", test_total_loss],[\"Classification Accuracy\", test_total_accuracy/test_counter]], \n",
    "                       headers=[\"Testing Loss/Accuracy\", \"Value\"]))\n",
    "    \n",
    "    \n",
    "    if test_total_accuracy/test_counter > best_accuracy :\n",
    "        \n",
    "        saver.save(sess, \"./cnn_ckpt/model.ckpt\")\n",
    "        print(\"Model saved !\")\n",
    "        best_accuracy = test_total_accuracy/test_counter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nasa",
   "language": "python",
   "name": "nasa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
